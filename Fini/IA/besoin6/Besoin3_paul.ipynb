{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61bbd2aa",
   "metadata": {},
   "source": [
    "Import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dff4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4873d106",
   "metadata": {},
   "source": [
    "création de la dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420875e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"After_Sort.csv\", parse_dates=[\"BaseDateTime\"])\n",
    "df.sort_values(\"BaseDateTime\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b1045d",
   "metadata": {},
   "source": [
    "print informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab3ce7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())\n",
    "print(df.info())\n",
    "print(df.describe())\n",
    "print(df[\"VesselType\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6be89a",
   "metadata": {},
   "source": [
    "ajout des colones lat et long +5, +10, +15 et delta_temps à la dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8768f914",
   "metadata": {},
   "outputs": [],
   "source": [
    "nouvelles_colonnes = ['LONG_5', 'LAT_5', 'LONG_10', 'LAT_10', 'LONG_15', 'LAT_15','Delta_temps']\n",
    "\n",
    "# Ajout des colonnes avec des valeurs vides (NaN)\n",
    "for col in nouvelles_colonnes:\n",
    "    df[col] = None  # ou pd.NA, ou np.nan si vous utilisez numpy\n",
    "\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e701a7f",
   "metadata": {},
   "source": [
    "Remplissage de la colonne Delta_temps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9464cebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Delta_temps'] = df.sort_values(['MMSI', 'BaseDateTime']).groupby('MMSI')['BaseDateTime'].diff().dt.total_seconds()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4ffa5a",
   "metadata": {},
   "source": [
    "OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc5b210",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "def remplir_positions(df):\n",
    "    df_result = []\n",
    "\n",
    "    # Trier les données\n",
    "    df = df.sort_values([\"MMSI\", \"BaseDateTime\"]).reset_index(drop=True)\n",
    "\n",
    "    for mmsi, group in df.groupby(\"MMSI\"):\n",
    "        group = group.sort_values(\"BaseDateTime\").reset_index(drop=True)\n",
    "        for idx, row in group.iterrows():\n",
    "            base_time = row[\"BaseDateTime\"]\n",
    "\n",
    "            # Fenêtres de temps en minutes\n",
    "            t_windows = {\n",
    "                '5': (base_time + timedelta(minutes=3), base_time + timedelta(minutes=7)),\n",
    "                '10': (base_time + timedelta(minutes=8), base_time + timedelta(minutes=12)),\n",
    "                '15': (base_time + timedelta(minutes=13), base_time + timedelta(minutes=17)),\n",
    "            }\n",
    "\n",
    "            positions = {}\n",
    "            for key, (start, end) in t_windows.items():\n",
    "                candidates = group[(group[\"BaseDateTime\"] >= start) & (group[\"BaseDateTime\"] <= end)]\n",
    "                if not candidates.empty:\n",
    "                    chosen = candidates.iloc[0]\n",
    "                    positions[f\"LAT_{key}\"] = chosen[\"LAT\"]\n",
    "                    positions[f\"LONG_{key}\"] = chosen[\"LON\"]\n",
    "                else:\n",
    "                    break  # Si une fenêtre n’a pas de donnée, on saute cette ligne\n",
    "            else:\n",
    "                df_result.append({**row, **positions})\n",
    "\n",
    "    return pd.DataFrame(df_result)\n",
    "\n",
    "# Appliquer sur la dataframe complète avant split\n",
    "df_cleaned = remplir_positions(df)\n",
    "\n",
    "# Affichage pour vérification\n",
    "print(df_cleaned.head())\n",
    "print(f\"Nombre de lignes après nettoyage : {len(df_cleaned)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7219b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupérer les MMSI uniques de df_cleaned\n",
    "mmsi_uniques = df_cleaned['MMSI'].dropna().unique()\n",
    "\n",
    "# Mélanger\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(mmsi_uniques)\n",
    "\n",
    "# Répartition MMSI\n",
    "n_train = 97\n",
    "n_test = 30\n",
    "n_val = 22\n",
    "\n",
    "mmsi_train = mmsi_uniques[:n_train]\n",
    "mmsi_test = mmsi_uniques[n_train:n_train + n_test]\n",
    "mmsi_val = mmsi_uniques[n_train + n_test:n_train + n_test + n_val]\n",
    "\n",
    "# Création des sous-ensembles\n",
    "df_train = df_cleaned[df_cleaned['MMSI'].isin(mmsi_train)]\n",
    "df_test = df_cleaned[df_cleaned['MMSI'].isin(mmsi_test)]\n",
    "df_val = df_cleaned[df_cleaned['MMSI'].isin(mmsi_val)]\n",
    "\n",
    "# Vérification\n",
    "print(\"Nouveaux ensembles à partir de df_cleaned :\")\n",
    "print(\"Train:\", df_train['MMSI'].nunique())\n",
    "print(\"Test :\", df_test['MMSI'].nunique())\n",
    "print(\"Val  :\", df_val['MMSI'].nunique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa85602",
   "metadata": {},
   "source": [
    "Séparation en bases d'apprentissage, test et validation par MMSI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55fde09",
   "metadata": {},
   "source": [
    "Observation de début de chaque df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771eaa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train.head(5))\n",
    "print(\"\\n\")\n",
    "print(\"\\n\")\n",
    "print(\"  ===============================================================  \")\n",
    "print(\"\\n\")\n",
    "print(\"\\n\")\n",
    "print(df_test.head(5))\n",
    "print(\"\\n\")\n",
    "print(\"\\n\")\n",
    "print(\"  ===============================================================  \")\n",
    "print(\"\\n\")\n",
    "print(\"\\n\")\n",
    "print(df_val.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24268a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.sort_values([\"MMSI\",\"BaseDateTime\"], inplace=True)\n",
    "print(df_train.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c849721b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Taille de l'ensemble d'apprentissage : {len(df_train)}\")\n",
    "print(f\"Taille de l'ensemble de test : {len(df_test)}\")\n",
    "print(f\"Taille de l'ensemble de validation : {len(df_val)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f5eefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.sort_values([\"MMSI\",\"BaseDateTime\"], inplace=True)\n",
    "print(df_train.head())\n",
    "# print(df_train.iloc[2950:3000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d49788c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features = ['SOG', 'COG', 'Heading', 'LAT', 'LON']\n",
    "X = df_train[features]\n",
    "y_lat = df_train['LAT_5']\n",
    "y_lon = df_train['LONG_5']\n",
    "\n",
    "model_lat = RandomForestRegressor(\n",
    "    n_estimators=20,\n",
    "    min_samples_leaf=5,\n",
    "    min_samples_split=5\n",
    ")\n",
    "model_lat.fit(X, y_lat)\n",
    "\n",
    "model_lon = RandomForestRegressor(\n",
    "    n_estimators=20,\n",
    "    min_samples_leaf=5,\n",
    "    min_samples_split=5\n",
    ")\n",
    "model_lon.fit(X, y_lon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1742ced6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Sauvegarde\n",
    "joblib.dump(model_lat, 'model5_lat.pkl')\n",
    "joblib.dump(model_lon, 'model5_lon.pkl')\n",
    "\n",
    "# model_lat = joblib.load('model_lat.pkl')\n",
    "# model_lon = joblib.load('model_lon.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3975d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "X_test = df_test[features]\n",
    "y_lat_test = df_test['LAT_5']\n",
    "y_lon_test = df_test['LONG_5']\n",
    "\n",
    "lat_pred = model_lat.predict(X_test)\n",
    "lon_pred = model_lon.predict(X_test)\n",
    "\n",
    "rmse_lat = np.sqrt(mean_squared_error(y_lat_test, lat_pred))\n",
    "rmse_lon = np.sqrt(mean_squared_error(y_lon_test, lon_pred))\n",
    "print(\"RMSE LAT:\", rmse_lat)\n",
    "print(\"RMSE LON:\", rmse_lon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b8ab28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "X_test = df_val[features]\n",
    "y_lat_test = df_val['LAT_5']\n",
    "y_lon_test = df_val['LONG_5']\n",
    "\n",
    "lat_pred = model_lat.predict(X_test)\n",
    "lon_pred = model_lon.predict(X_test)\n",
    "\n",
    "rmse_lat = np.sqrt(mean_squared_error(y_lat_test, lat_pred))\n",
    "rmse_lon = np.sqrt(mean_squared_error(y_lon_test, lon_pred))\n",
    "print(\"RMSE LAT:\", rmse_lat)\n",
    "print(\"RMSE LON:\", rmse_lon)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ee686d",
   "metadata": {},
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bd55fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['SOG', 'COG', 'Heading','LAT', 'LON']\n",
    "X = df_train[features]\n",
    "y_lat = df_train['LAT_10']\n",
    "y_lon = df_train['LONG_10']\n",
    "\n",
    "model_lat10 = RandomForestRegressor(\n",
    "    n_estimators=20,\n",
    "    min_samples_leaf=5,\n",
    "    min_samples_split=5)\n",
    "model_lat10.fit(X, y_lat)\n",
    "\n",
    "model_lon10 = RandomForestRegressor(\n",
    "    n_estimators=20,\n",
    "    min_samples_leaf=5,\n",
    "    min_samples_split=5)\n",
    "model_lon10.fit(X, y_lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9373516",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Sauvegarde\n",
    "joblib.dump(model_lat, 'model10_lat.pkl')\n",
    "joblib.dump(model_lon, 'model10_lon.pkl')\n",
    "\n",
    "# model_lat = joblib.load('model_lat.pkl')\n",
    "# model_lon = joblib.load('model_lon.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e5fb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "X_test = df_test[features]\n",
    "y_lat_test = df_test['LAT_10']\n",
    "y_lon_test = df_test['LONG_10']\n",
    "\n",
    "lat_pred = model_lat.predict(X_test)\n",
    "lon_pred = model_lon.predict(X_test)\n",
    "\n",
    "rmse_lat = np.sqrt(mean_squared_error(y_lat_test, lat_pred))\n",
    "rmse_lon = np.sqrt(mean_squared_error(y_lon_test, lon_pred))\n",
    "print(\"RMSE LAT:\", rmse_lat)\n",
    "print(\"RMSE LON:\", rmse_lon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f853e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df_val[features]\n",
    "y_lat_test = df_val['LAT_10']\n",
    "y_lon_test = df_val['LONG_10']\n",
    "\n",
    "lat_pred = model_lat.predict(X_test)\n",
    "lon_pred = model_lon.predict(X_test)\n",
    "\n",
    "rmse_lat = np.sqrt(mean_squared_error(y_lat_test, lat_pred))\n",
    "rmse_lon = np.sqrt(mean_squared_error(y_lon_test, lon_pred))\n",
    "print(\"RMSE LAT:\", rmse_lat)\n",
    "print(\"RMSE LON:\", rmse_lon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd99cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['SOG', 'COG', 'Heading', 'LAT', 'LON']\n",
    "X = df_train[features]\n",
    "y_lat = df_train['LAT_15']\n",
    "y_lon = df_train['LONG_15']\n",
    "\n",
    "model_lat15 = RandomForestRegressor(\n",
    "    n_estimators=20,\n",
    "    min_samples_leaf=5,\n",
    "    min_samples_split=5\n",
    ")\n",
    "model_lat15.fit(X, y_lat)\n",
    "\n",
    "model_lon15 = RandomForestRegressor(   \n",
    "    n_estimators=20,\n",
    "    min_samples_leaf=5,\n",
    "    min_samples_split=5)\n",
    "model_lon15.fit(X, y_lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706dabda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde\n",
    "joblib.dump(model_lat, 'model15_lat.pkl')\n",
    "joblib.dump(model_lon, 'model15_lon.pkl')\n",
    "\n",
    "# model_lat = joblib.load('model_lat.pkl')\n",
    "# model_lon = joblib.load('model_lon.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f025292",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df_test[features]\n",
    "y_lat_test = df_test['LAT_15']\n",
    "y_lon_test = df_test['LONG_15']\n",
    "\n",
    "lat_pred = model_lat.predict(X_test)\n",
    "lon_pred = model_lon.predict(X_test)\n",
    "\n",
    "rmse_lat = np.sqrt(mean_squared_error(y_lat_test, lat_pred))\n",
    "rmse_lon = np.sqrt(mean_squared_error(y_lon_test, lon_pred))\n",
    "print(\"RMSE LAT:\", rmse_lat)\n",
    "print(\"RMSE LON:\", rmse_lon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb668ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df_val[features] \n",
    "y_lat_test = df_val['LAT_15']\n",
    "y_lon_test = df_val['LONG_15']\n",
    "\n",
    "lat_pred = model_lat.predict(X_test)\n",
    "lon_pred = model_lon.predict(X_test)\n",
    "\n",
    "rmse_lat = np.sqrt(mean_squared_error(y_lat_test, lat_pred))\n",
    "rmse_lon = np.sqrt(mean_squared_error(y_lon_test, lon_pred))\n",
    "print(\"RMSE LAT:\", rmse_lat)\n",
    "print(\"RMSE LON:\", rmse_lon)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a171d1",
   "metadata": {},
   "source": [
    "prendre entre 295 et 305sec\n",
    "prendre entre 595 et 605sec\n",
    "prendre entre 895 et 905sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44602cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpoler_positions_dataframe(df):\n",
    "    \"\"\"\n",
    "    Fonction pour interpoler les positions à 5, 10 et 15 minutes\n",
    "    pour un dataframe donné\n",
    "    \"\"\"\n",
    "    # Créer une copie pour éviter les warnings\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Trier par MMSI et datetime pour assurer l'ordre chronologique\n",
    "    df_copy = df_copy.sort_values(['MMSI', 'BaseDateTime']).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Début de l'interpolation pour {len(df_copy)} lignes...\")\n",
    "    \n",
    "    # Parcourir chaque navire\n",
    "    for mmsi in df_copy['MMSI'].unique():\n",
    "        # Filtrer les données du navire actuel\n",
    "        mask_navire = df_copy['MMSI'] == mmsi\n",
    "        indices_navire = df_copy[mask_navire].index.tolist()\n",
    "        \n",
    "        # Parcourir les positions de ce navire\n",
    "        for i in range(len(indices_navire) - 1):\n",
    "            idx_actuel = indices_navire[i]\n",
    "            idx_suivant = indices_navire[i + 1]\n",
    "            \n",
    "            # Récupérer les positions et temps\n",
    "            lat_A = df_copy.loc[idx_actuel, 'LAT']\n",
    "            lon_A = df_copy.loc[idx_actuel, 'LON']\n",
    "            time_A = df_copy.loc[idx_actuel, 'BaseDateTime']\n",
    "            \n",
    "            lat_B = df_copy.loc[idx_suivant, 'LAT']\n",
    "            lon_B = df_copy.loc[idx_suivant, 'LON']\n",
    "            time_B = df_copy.loc[idx_suivant, 'BaseDateTime']\n",
    "            \n",
    "            # Calculer l'intervalle de temps en secondes\n",
    "            delta_temps = (time_B - time_A).total_seconds()\n",
    "            \n",
    "            # Interpolation à 5 minutes (300s)\n",
    "            if delta_temps >= 300:\n",
    "                fraction_5min = 300 / delta_temps\n",
    "                lat_5min = lat_A + fraction_5min * (lat_B - lat_A)\n",
    "                lon_5min = lon_A + fraction_5min * (lon_B - lon_A)\n",
    "                \n",
    "                df_copy.loc[idx_actuel, 'LAT_5'] = lat_5min\n",
    "                df_copy.loc[idx_actuel, 'LONG_5'] = lon_5min\n",
    "            \n",
    "            # Interpolation à 10 minutes (600s)\n",
    "            if delta_temps >= 600:\n",
    "                fraction_10min = 600 / delta_temps\n",
    "                lat_10min = lat_A + fraction_10min * (lat_B - lat_A)\n",
    "                lon_10min = lon_A + fraction_10min * (lon_B - lon_A)\n",
    "                \n",
    "                df_copy.loc[idx_actuel, 'LAT_10'] = lat_10min\n",
    "                df_copy.loc[idx_actuel, 'LONG_10'] = lon_10min\n",
    "            \n",
    "            # Interpolation à 15 minutes (900s)\n",
    "            if delta_temps >= 900:\n",
    "                fraction_15min = 900 / delta_temps\n",
    "                lat_15min = lat_A + fraction_15min * (lat_B - lat_A)\n",
    "                lon_15min = lon_A + fraction_15min * (lon_B - lon_A)\n",
    "                \n",
    "                df_copy.loc[idx_actuel, 'LAT_15'] = lat_15min\n",
    "                df_copy.loc[idx_actuel, 'LONG_15'] = lon_15min\n",
    "    \n",
    "    print(\"Interpolation terminée !\")\n",
    "    return df_copy\n",
    "\n",
    "# Application de l'interpolation sur les trois ensembles\n",
    "print(\"=== INTERPOLATION DE L'ENSEMBLE D'APPRENTISSAGE ===\")\n",
    "df_train_interpolated = interpoler_positions_dataframe(df_train)\n",
    "\n",
    "print(\"\\n=== INTERPOLATION DE L'ENSEMBLE DE TEST ===\")\n",
    "df_test_interpolated = interpoler_positions_dataframe(df_test)\n",
    "\n",
    "print(\"\\n=== INTERPOLATION DE L'ENSEMBLE DE VALIDATION ===\")\n",
    "df_val_interpolated = interpoler_positions_dataframe(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad118743",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verifier_interpolation(df, nom_ensemble):\n",
    "    \"\"\"\n",
    "    Fonction pour vérifier les résultats de l'interpolation\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== VÉRIFICATION - {nom_ensemble} ===\")\n",
    "    \n",
    "    # Compter les valeurs non nulles pour chaque colonne d'interpolation\n",
    "    interpolations_5min = df['LAT_5'].notna().sum()\n",
    "    interpolations_10min = df['LAT_10'].notna().sum()\n",
    "    interpolations_15min = df['LAT_15'].notna().sum()\n",
    "    \n",
    "    print(f\"Interpolations à 5 minutes : {interpolations_5min}\")\n",
    "    print(f\"Interpolations à 10 minutes : {interpolations_10min}\")\n",
    "    print(f\"Interpolations à 15 minutes : {interpolations_15min}\")\n",
    "    \n",
    "    # Afficher quelques exemples d'interpolation\n",
    "    exemples = df[df['LAT_5'].notna()].head(3)\n",
    "    if len(exemples) > 0:\n",
    "        print(f\"\\nExemples d'interpolation à 5 minutes :\")\n",
    "        for idx, row in exemples.iterrows():\n",
    "            print(f\"MMSI {row['MMSI']} : Position originale ({row['LAT']:.5f}, {row['LON']:.5f}) -> Position à +5min ({row['LAT_5']:.5f}, {row['LONG_5']:.5f})\")\n",
    "\n",
    "# Vérification des trois ensembles\n",
    "verifier_interpolation(df_train_interpolated, \"TRAIN\")\n",
    "verifier_interpolation(df_test_interpolated, \"TEST\")\n",
    "verifier_interpolation(df_val_interpolated, \"VALIDATION\")\n",
    "\n",
    "print(\"\\n=== INTERPOLATION TERMINÉE POUR TOUS LES ENSEMBLES ===\")\n",
    "print(\"Les DataFrames df_train_interpolated, df_test_interpolated et df_val_interpolated\")\n",
    "print(\"contiennent maintenant les positions interpolées dans les colonnes LAT_5, LONG_5, LAT_10, LONG_10, LAT_15, LONG_15\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12511e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "seuil_SOG = 0.5  # à ajuster selon ton dataset\n",
    "df_filtre = df[df['SOG'] > seuil_SOG].reset_index(drop=True)\n",
    "len(df_filtre)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
